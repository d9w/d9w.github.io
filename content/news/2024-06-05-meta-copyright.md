---
title: Meta wants to train AI on your data
author: Dennis G. Wilson
date: '2024-06-06'
tags:
  - LLM
  - generative ai
  - policy
  - newsletter
---

## A shift from web scraping to user datasets for gen AI

Last week, me and other European users of Meta's products got an email saying that they're "getting ready to expand our AI at Meta experiences to [our] region." Starting June 26, Meta will be training AI models on user data from (at least) Facebook and Instagram for users who didn't opt out. This has driven a number of creators on Instagram to [look for alternatives](https://www.independent.co.uk/tech/cara-art-app-instagram-ai-b2556592.html) in protest of the new policy. Visual artists in particular have long had concerns over [training AI on their work](https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-copyright-1849388060), which this new policy would explicitly allow.

In the fierce AI competition between Microsoft, Google, and Meta, there is a clear interest in using the vast amount of data on these platforms for AI training. Meta has started offering [AI-generated ads](https://www.reuters.com/technology/meta-expand-ai-image-generation-offerings-ads-2024-05-07/) in a push to make revenue from their [very expensive](https://www.reuters.com/technology/meta-raises-2024-expenses-forecast-support-ai-development-2024-04-24/) foray into generative AI. The push from Meta to get explicit consent of their user data from training AI is part of a shift away from web-scraped datasets for AI and towards privately owned web-scale datasets. In this post, I'll cover the motivations for this shift, mostly from a privacy and copyright perspective. Big disclaimer upfront that I'm a technical AI researcher, and not a lawyer, so be sure to season my opinions appropriately.

Generative AI is intensely data-hungry. I've covered training data for [large language models](https://goodcomputer.substack.com/p/an-introduction-to-large-language) previously, where enormous public datasets were gathered through web-scraping to feed these massive models. Much of the pushback against this new policy has been focused on image generation models, however. Image generation models seen online like Midjourney or Dall-E are [text-to-image](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf) models based on [diffusion](https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf). Compared to the previous generation of image generation models, [Generative Adversarial Networks (GANs)](https://dl.acm.org/doi/pdf/10.1145/3422622), models based on diffusion can scale to much larger datasets and more complex mappings, like text to images. I could do a whole post on diffusion, which is also used for things like [weather forecasting](https://www.science.org/doi/pdf/10.1126/sciadv.adk4489) in another post; let me know if that'd be of interest in the comments. 

While diffusion was published in 2019, very large, human-annotated datasets of images weren't available, at least publicly. So, like they did for LLMs, AI companies turned to web scraping. One prominent example is the [LAION-5B](https://laion.ai/) dataset, which has 5.85 billion image-text pairs sourced from [Common Crawl](https://commoncrawl.org/), a non-profit organization that offers downloads of the public-facing web. However, scraping the whole public-facing web gives you a lot of things, including [copyrighted images](https://www.theverge.com/2023/12/4/23988403/getty-lawsuit-stability-ai-copyright-infringement), [private medical information](https://arstechnica.com/information-technology/2022/09/artist-finds-private-medical-record-photos-in-popular-ai-training-data-set/) and depictions of [child abuse](https://cyber.fsi.stanford.edu/io/news/investigation-finds-ai-image-generation-models-trained-child-abuse). The uncurated nature of such web-scraped datasets poses ethical and legal challenges, as they can include inappropriate or illegal content. There are two main problems: privacy and copyright. While the legal landscape surrounding AI data usage is complex and evolving, some of these are already well-established and make a clear case against training AI models on web-scraped datasets, especially for data privacy.

The international standard of data privacy laws is the [General Data Protection Regulation (GDPR) in Europe](https://gdpr.eu/). Since its start in 2018, the GDPR has resulted in [â‚¬4.5 billion of fines](https://www.enforcementtracker.com/?insights). One of the fundamental principles of the GDPR is user consent; users have to consent to how their personal data is being used. If your personal data was scraped from a site where you gave consent, and ended up on a site without you knowing about it, that new site is in violation of the GDPR. LAION, which is based in Germany, proposes a [takedown request](https://laion.ai/privacy-policy/) if you don't want your face or medical scans to be in their dataset. However, models trained on the dataset before the image was removed will still have the personal information encoded in the weights of the network. My hope is that the [European AI Act](https://goodcomputer.substack.com/p/european-union-ai-act) helps clarify that in the future, but there is a clear issue of privacy in training.

GDPR compliance is the first argument for companies to ask users for AI training consent, and it has the clearest legal precedent. Under the GDPR, new forms of manipulation of personal data require new consent, and users must always be able to opt out of the various ways their personal data is being manipulated. However, this only applies to *personal* data, which are things like name, date of birth, and other identifying aspects. Some of Instagram posts may constitute personal data, but that's not what's making artists worry about this new policy.

The second big issue is copyright. This could also be the subject of a complete post later, but briefly, the question of training AI on copyrighted data without the consent of the copyright holder is under fierce debate in court right now. There's [over 20 ongoing lawsuits](https://web.archive.org/web/20240407000139/https://www.thefashionlaw.com/from-chatgpt-to-deepfake-creating-apps-a-running-list-of-key-ai-lawsuits/) about training on copyrighted data, with plantiffs including [Getty Images](https://www.theverge.com/2023/12/4/23988403/getty-lawsuit-stability-ai-copyright-infringement), [authors like George R. R. Martin](https://www.classaction.org/media/authors-guild-et-al-v-openai-inc-et-al.pdf) and [the New York Times](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html). Some of these cases have started litigation, but there is [no clear verdict yet](https://www.loeb.com/en/insights/publications/2023/11/andersen-v-stability-ai-ltd), and there will probably be [conflicting verdicts in different cases](https://www.reuters.com/legal/legalindustry/judge-denies-plaintiffs-effort-intervene-new-york-copyright-actions-against-2024-04-17/). The protections from copyright law concerning AI training will take years to be fully worked out in court, but there is good reason to believe that generative AI training runs afoul of US copyright law.

However, if users give their explicit consent to their data being used to train AI on platforms like Instagram, it becomes much harder to argue that their copyright or data privacy have been violated. This marks a shift compared to the main recent trend where new starters like OpenAI and LAION created massive datasets through public web scraping, even [scraping from other companies like Google's Youtube](https://www.theverge.com/2024/4/6/24122915/openai-youtube-transcripts-gpt-4-training-data-google). Instead, the new direction is large companies, which already have vast user datasets, using them for AI training. And it isn't just Meta: Reddit has deals [with Google](https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/) and [with OpenAI](https://openai.com/index/openai-and-reddit-partnership/) to allow training on user data, and news organizations like [Vox media](https://www.axios.com/2024/05/29/atlantic-vox-media-openai-licensing-deal) have licensed their work to AI companies.

Web scraping for AI training was always ethically unadvisable and often done with the excuse [that it was only for research](https://openreview.net/forum?id=M3Y74vmsMcY) and not for commercial products. In 2024, that argument is clearly false, and the shift towards big tech policies on AI training isn't surprising. However, this new trend of blanket agreements that user data can all be used for AI training is also concerning to me.

First, this shift is happening with little true user consent. Users have reported that the opt out form for Meta's new policy is incredibly [frustrating to fill out or even find](https://www.fastcompany.com/91132854/instagram-training-ai-on-your-data-its-nearly-impossible-to-opt-out), and most weren't aware of the new policy until the notification to European users last week. Reddit's decision to sell its user data is retroactive, meaning that the many years of user data already stored on the site will be sold without the consent of those users.

Second, this shift will degrade data transparency. The one advantage of public databases like LAION is that they're public. Their contents can be studied and the flaws revealed. A website that allows for searching the datasets, [https://haveibeentrained.com/](https://haveibeentrained.com/), has been fundamental in court cases to demonstrate privacy and copyright violations. With private datasets made of Instagram posts or Reddit comments (not all of which are public-facing), this transparency becomes much more difficult.

As this trend continues, expect to see most platforms include explicit right to AI model training based on user data. Those who care about their privacy or the right to the content they put online are left with little recourse. I hope that [Europeans will be mostly shielded](https://www.edps.europa.eu/system/files/2024-06/24-06-03_genai_orientations_en.pdf) by the GDPR and able to opt out based on privacy laws. Platforms like [Cara](https://cara.app/) are getting a lot of attention because they have a clear stance against AI training. Some artists are using tools like [Glaze and Nightshade](https://glaze.cs.uchicago.edu/) which provide invisible, AI-proof watermarks. Opting out, changing platforms, and post-processing data to protect it are all actions that users have to take to protect their data, though. Without action, the default for the web, at least for the near future, seems to be a feeding ground for hungry generative AI.
